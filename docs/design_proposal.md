# Design proposal

## Functionality
Generating audio samples conditioned on descriptive text captions 
 - We want to create simple app that would give the user possibility to generate music based on text with different models and parameters 
 - Frontend for non-programming users 
 - Simple API that would give the user ability to generate music using different models 
 - Implementing metrics calculations for generated audio 
 - We strive for scalable architecture in which addition of new models or modalities should be easy 

## Technology stack

 * Gradio 
 * Python 
    - Librosa 
    - Pytest 
    - Pylint/flake8 
    - Pytorch/tensorflow 
    - Black 
    - venv 
* Github actions 
## Experiments
 * With the use of our framework, we want to do the following: 
    - Subjective comparison of the music generated by different models with the same prompt 
    - Check in which types of music specific models are good 
    - Check how metrics behave in different models for the same prompt 
    - Check how model behaves depending on prompt 

## Project roadmap
| Week | Milestones |
|---|---|
| 11.03.2024 - 17.03.2024 | project setup |
| 18.03.2024 - 24.03.2024  | research of papers |
| 25.03.2024 - 31.03.2024  | simple implementation of one model |
| 01.04.2024 - 07.04.2024  | gradio app |
| 08.04.2024 - 14.04.2024  | metrics implementation |
| 15.03.2024 - 21.04.2024  | implementation of second model |
| 22.03.2024 - 28.04.2024  | user oriented experiments |
| 29.03.2024 - 05.05.2024  | architecture overview (looking for design patterns that could be used) |
| 06.05.2024 - 12.05.2024  | implementation of other models |
| 13.05.2024 - 19.05.2024  | experiments |
| 20.05.2024 - 26.05.2024  | code refactoring |
| 27.05.2024 - 02.06.2024  | deployment setup |
| 03.06.2024 - 09.06.2024  | documentation |
| 10.06.2024 - 16.06.2024  | finishing, last touches, buffer for vis maior |

## References

[1] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D ́efossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation, 2023. (https://arxiv.org/pdf/2301.11757.pdf ) 

[2] Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Zhifeng Chen, and Wei Han. Noise2music: Text-conditioned music generation with diffusion models, 2023. (https://arxiv.org/pdf/2301.11325.pdf)

[3] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. Mulan: A joint embedding of music audio and natural language, 2022. (https://arxiv.org/pdf/2302.03917.pdf )

[4] Andrea Agostinelli, Timo I. Denk, Zal ́an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023. (https://arxiv.org/pdf/2208.12415.pdf )

[5] Flavio Schneider, Ojasv Kamal, Zhijing Jin, and Bernhard Sch ̈olkopf. Moˆusai: Text-to-music generation with long-context latent diffusion, 2023. (https://arxiv.org/pdf/2209.15352.pdf )


 
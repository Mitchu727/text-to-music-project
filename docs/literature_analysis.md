| Model | Paper | Github | Can be run locally | Evaluation methods | Compute power needed |
|---|---|---|---|---|---|
| MusicGen | [arxiv](https://arxiv.org/abs/2306.05284) | [github](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md) | running through the api fails on windows but is possible run with the use of transformers library | Evaluated on MusicCaps from https://arxiv.org/abs/2301.11325 using Fréchet Audio Distance,  Kullback-Leiber Divergence and CLAP (section 3.3 of the paper), it was also evaluated using subjective metrics which are overall quality (OVL) and relevance to the input text (REL). The paper also proposes a new metric called chroma cosine-similarity (section 4.2) | To run: 16 gb GPU needed for larger models (according to github documentation) To train: 32, 64, 96 unspecified GPUs (according to paper)|
| AudioLDM | [arxiv](https://arxiv.org/abs/2301.12503) | [github](https://github.com/haoheliu/AudioLDM) | Gradio app: fails with error `AttributeError: module 'gradio' has no attribute 'Box'` Api and command line: Failed with error: `Torch not compiled with CUDA enabled` | The model was evaluated on Audioset (10% randomly selected samples, and concatenated labels as text description) and Audio Caps (one from 5 text captions is chosen randomly). The evaluation was both subjective and objective. As obejctive metrics Frechet Distance, inception score, Kullback-Leiber Divergence and Fréchet Audio Distance were usesd, as subjective - OVL and REL. Authors wrote that all those metrics were built on PANN (however I do not fully understand what that should mean) | To run: 8gb GPU, 64bit OS (according to github documentation) To train: 1 x RTX 3090 or A100 |
| AudioLDM2 | [arxiv](https://arxiv.org/abs/2308.05734) | [github](https://github.com/haoheliu/audioldm2) | ? | ? | ? |
| AudioGen | [arxiv](https://arxiv.org/abs/2209.15352) | ? | ? | ? | ? |
| Noise2Music | [arxiv](https://arxiv.org/pdf/2302.03917.pdf) | NOT PROVIDED | NO | Model parameters were chosen based on genarated results quality. Evaluation were conducted on 16kHz waveforms. To measure the quality of generation, authors used two kinds of metrics: the Frechet Audio Distance (FAD) and the MuLan similarity score. Metrics were calculated for these three datasets: MagnaTagATune (MTAT), AudioSet-Music-Eval and MusicCaps  | Inference time for 4 Google Cloud TPU V4 with GSPMD applied (to partition the model, time reduced by more than 50%) ~151s |
| Mulan | [arxiv](https://arxiv.org/pdf/2208.12415.pdf) | NOT PROVIDED | NO | Paper was added as additional source for reaserch about shared embeddings between audio and text, therefore it probably won't be implemented by us | X |
| MusicLm | [arxiv](https://arxiv.org/pdf/2301.11325.pdf) | ? | ? | ? | ? |
| Moûsai | [arxiv](https://arxiv.org/pdf/2301.11757.pdf) | ? | ? | ? | ? |
| JEN-1, JEN-1 COMPOSER (Preprints) | [arxiv](https://arxiv.org/abs/2308.04729) [arxiv](https://arxiv.org/abs/2310.19180) | NOT PROVIDED | No, but we should observe future developement because it achieves SOTA performance and authors may publish articles with code before our project ends | Objective evaluation includes three metrics: Frechet Audio Distance (FAD), Kullback-Leibler Divergence (KL) and CLAP score. Additionally perceptual evaluation was conducted, measuring two key aspects of generated music: text-to-music quality (T2M-QLT) and alignment to the text input (T2M-ALI). Both of them were rated in scale 1 to 100 by human raters. In case of COMPOSER authors used mixed CLAP score and Relative Preference Ratio (RPR) from human evaluation | 8 A100 GPUs for training for JEN-1 and 2 A100 for COMPOSER | 

Furhtermore it is worth to observe this repo: https://github.com/archinetai/audio-ai-timeline
